# Integer Sequence Learning

## Problem Statement
This problem is defined by [Kaggle](https://www.kaggle.com/ "Click to visit") team's competition named [Integer Sequence Learning](https://www.kaggle.com/c/integer-sequence-learning "Click to visit"). It challenges you create a machine learning algorithm capable of guessing the next number in an integer sequence. While this sounds like pattern recognition in its most basic form, a quick look at the data will convince you this is anything but basic!

The [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network "Click to check") approach – usually just called "RNNs" - is applied to solve this problem. This task particularly interests me as it's analogous to word prediction. Hence integers are treated as words in the solution.
    
## Datasets 

The dataset ([download here](https://www.kaggle.com/c/integer-sequence-learning/data "Click to download dataset")) of this project contains the majority of the integer sequences from the On-Line Encyclopedia of Integer Sequences® (OEIS®). It is split into a training set, where you are given the full sequence, and a test set, where we have removed the last number from the sequence. The task is to predict this removed integer.

Note that some sequences may have identical beginnings (or even be identical altogether). They have not been removed these from the dataset.

## Requirements

    Python >= 2.7.0
    numpy >= 1.11.1
    Keras >= 2.0.0

## File Descriptions
- train.csv - the training set, contains full sequences
- test.csv - the test set, missing the last number in each sequence

## Running Flow
- Step 1: Download dataset and put in `data` folder
- Step 2: Run `prepro.py` to preprocess data
- Step 3: Run `train.py` to train data
- Step 4: Run `result.py` to get the final prediction result `result.csv`
 
## Model Evaluation and Validation
The metrics discussed at the beginning of the investigation (the prdiction accuracy, i.e., the percentage of sequences where the next number is predicted correctly) can be applied to judge the health and validate the designed model.

Based on multiple experiments, the final model used to implement predition in terms of architecture and hyperparameters is "Inputs -> GRU Layer 1 of 1000 hidden units -> Dropout -> GRU Layer 2 of 1000 hidden units -> Dropout -> Time distributed dense -> Outputs".

As for the test set the final element has been removed, which is the target we are trying to predict. Based on above model, we will try to generate the last item of each sequence of the test set.

The big question of this investigation, is whether this model can be used to predict the last term of a given sequence accurately. As the nature of the challenge was a contest, the predictions created by the model were submitted online on Kaggle for a blind evaluation and received a decent score `0.13477` which is ranking around top 30%.

![](https://github.com/theodoreguo/Kaggle/blob/master/Integer%20Sequence%20Learning/Images/Kaggle%20Integer%20Sequence%20Learning%20Submission.png)

Kaggle's evaluation system gave a well formed tests covering a variety of test cases (113,845 scenarios), which is a objective way to test and prove the robustness of the model. Therefore, given the score generated by Kaggle, we can say that the model does generalize well to unseen data and it's not sensitive to small changes in the data or to outliers. Essentially, we trust this model as well as its predition results.

## Justification
In the Benchmark section the `Mode` methodology is discussed as the benchmark model for the last number prediction in a certain sequence. Compared with the Mode Benchmark with an accuracy of `0.05746`, the predictions generated by the designed model can be regarded as meeting the specification considering only the top 20 accuracy scores of the competition leaderboard are greater than 0.20.
